{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data from Twitter\n",
    "The following block will use Selenium to scrape tweets from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinity_scroll(driver,filename):\n",
    "    try:\n",
    "        for i in range(1000):\n",
    "            \n",
    "            soupObj = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            \n",
    "            with open(filename,'a') as f: #remember to delete the file everytime you run it.\n",
    "                f.write(str(soupObj))\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    except:\n",
    "        return\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "def scrape_tweets(twitter_handle,from_date,to_date,filename):\n",
    "    \n",
    "    url = f'https://twitter.com/search?q=(from%3A{twitter_handle})%20until%3A{to_date}%20since%3A{from_date}%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=live'\n",
    "    response = requests.get(url)\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    time.sleep(10) \n",
    "    \n",
    "    # scrolls down to the bottom of the page\n",
    "    infinity_scroll(driver,filename)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_handle = 'gsk'\n",
    "# twitter_handle = 'sanofi'\n",
    "# twitter handle = 'AstraZeneca'\n",
    "twitter_handle = 'realdonaldtrump'\n",
    "\n",
    "# twitter_handle = 'elonmusk'\n",
    "from_date = '2020-01-01'\n",
    "to_date = '2020-10-01'\n",
    "\n",
    "\n",
    "scrape_tweets(twitter_handle,from_date,to_date,'trump_tweets_Jan_2020_Sept_30_2020.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Tweet Data\n",
    "The following code processes the data pulled from the XML file to create a listing of all Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets2(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        soupObj = BeautifulSoup(f,'html.parser')\n",
    "    \n",
    "    total_tweets = len(soupObj.findAll(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\"))\n",
    "    tweet_listing = []\n",
    "    time_listing = []\n",
    "    \n",
    "    tweet_block = soupObj.find(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\")\n",
    "    \n",
    "    for i in range(total_tweets):\n",
    "        try:\n",
    "            \n",
    "            tweet = tweet_block.find('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "            tweet_listing.append(tweet.text)\n",
    "        \n",
    "        except AttributeError:\n",
    "            tweet_listing.append(np.NaN)\n",
    "        \n",
    "        try:\n",
    "            timestamp = tweet_block.find('time')\n",
    "            time_listing.append((timestamp.find_next('time').attrs)['datetime'])\n",
    "        \n",
    "        except AttributeError:\n",
    "            time_listing.append(np.NaN)\n",
    "\n",
    "        tweet_block = tweet_block.find_next(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\")\n",
    "\n",
    "    return pd.DataFrame({'Time':time_listing,'Tweet':tweet_listing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'astrazeneca_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'gsk_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'pfizer_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'elon_tweets_Sept_2019_Sept_2020.xml'\n",
    "filepath = 'trump_tweets_Jan_2020_Sept_30_2020.xml'\n",
    "\n",
    "df = get_tweets2(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL ADJUSTMENT\n",
    "## Not sure why but all the tweets scraped are offset by 1 with the first date value missing.\n",
    "## In the interest of time, I'm shifting the cells and manually adding it back:\n",
    "\n",
    "df_copy['Time'] = df_copy[['Time']].shift()\n",
    "df_copy.loc[[0,0],'Time'] = \"2020-09-30T23:48:24.000Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-30T23:48:24.000Z</td>\n",
       "      <td>Leaving Minneapolis for a quick stop in Duluth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-30T21:50:51.000Z</td>\n",
       "      <td>Just landed in Minnesota. Hasn’t been won by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-30T21:16:54.000Z</td>\n",
       "      <td>HIGHEST CABLE TELEVISION RATINGS OF ALL TIME. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-30T20:27:35.000Z</td>\n",
       "      <td>So weird to watch @FoxNews interviewing only f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-30T18:25:56.000Z</td>\n",
       "      <td>Radical Left Democrats are going CRAZY!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Time                                              Tweet\n",
       "0  2020-09-30T23:48:24.000Z  Leaving Minneapolis for a quick stop in Duluth...\n",
       "1  2020-09-30T21:50:51.000Z  Just landed in Minnesota. Hasn’t been won by a...\n",
       "2  2020-09-30T21:16:54.000Z  HIGHEST CABLE TELEVISION RATINGS OF ALL TIME. ...\n",
       "3  2020-09-30T20:27:35.000Z  So weird to watch @FoxNews interviewing only f...\n",
       "4  2020-09-30T18:25:56.000Z            Radical Left Democrats are going CRAZY!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('gsk_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('pfizer_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('astrazeneca_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('elon_tweets_Sept_2019_Sept_2020.csv')\n",
    "\n",
    "df_copy.to_csv('trump_tweets_Jan_2020_Sep_30_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GetOldTweets3 no longer works since twitter changed the endpoint. Only been an issue for 4 days, but may be fixed.\n",
    "# save code incase the developers fix it.\n",
    "# import GetOldTweets3 as got\n",
    "# tweetCriteria = got.manager.TweetCriteria().setQuerySearch(\"realDonaldTrump\").setMaxTweets(2)\n",
    "# tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used in testing\n",
    "# variables for testing scrape_tweets()\n",
    "# 'https://twitter.com/search?q=(from%3Arealdonaldtrump)%20until%3A2020-07-01%20since%3A2020-01-01%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=live'\n",
    "# twitter_handle = 'realdonaldtrump'\n",
    "# from_date = '2020-01-01'\n",
    "# to_date = '2020-07-01'\n",
    "\n",
    "# other scrapers:\n",
    "# snscrape's jsonl\n",
    "# https://github.com/twintproject/twint/issues/918#issuecomment-696448934"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
